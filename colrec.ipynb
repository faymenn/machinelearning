{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program implements a recommender system based on the collaborative filtering algorithm. The program inputs a matrix Y which contains users as coloumns, and movies as rows and the matrix elements are the ratings of the movie given in increments of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4777, 443)\n",
      "(4777, 443)\n"
     ]
    }
   ],
   "source": [
    "#Inputs the ratings matrix of the users and the movies they have rated\n",
    "dfY = pd.read_csv(\"C:/Users/Fnu Aymen/Documents/Machine Learning Data/colrec/small_movies_Y.csv\")\n",
    "Y = dfY.to_numpy(dtype=float)\n",
    "\n",
    "#Contains a binary matrix consisting of whethe or not a user has given a rating\n",
    "dfR = pd.read_csv(\"C:/Users/Fnu Aymen/Documents/Machine Learning Data/colrec/small_movies_R.csv\")\n",
    "R = dfR.to_numpy(dtype=float)\n",
    "\n",
    "print(Y.shape)\n",
    "#contains 4777 movies rated by 443 users\n",
    "\n",
    "print(R.shape)\n",
    "\n",
    "num_movies, num_users = Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function for the collaborative filtering algorithm.\n",
    "\n",
    "def cost(Y,X,W,b,R,l):\n",
    "    \n",
    "    J = 0\n",
    "    \n",
    "    j = (X @ tf.transpose(W) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (l/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2)) #reduce_sum is used to sum the elements of the tensor\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ideally we should normalize the ratings matrix Y across the columns (i.e. per-user) but this is not done in the code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 100\n",
    "\n",
    "#Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 2514383.4\n",
      "Training loss at iteration 20: 145643.3\n",
      "Training loss at iteration 40: 58209.5\n",
      "Training loss at iteration 60: 29058.9\n",
      "Training loss at iteration 80: 16980.1\n",
      "Training loss at iteration 100: 11209.3\n",
      "Training loss at iteration 120: 8136.0\n",
      "Training loss at iteration 140: 6360.1\n",
      "Training loss at iteration 160: 5264.2\n",
      "Training loss at iteration 180: 4548.5\n"
     ]
    }
   ],
   "source": [
    "iterations = 200\n",
    "l = 1\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFlowâ€™s GradientTape\n",
    "    # to record the operations used to compute the cost \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Compute the cost (forward pass included in cost)\n",
    "        cost_value = cost(Y,X,W,b,R,l)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")\n",
    "\n",
    "\n",
    "#Number of iterations,number of features and l can be changed to optimise the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have the optimized parameters for X, W and b. To make predictions we use the following formula:\n",
    "\n",
    "pred = X @ tf.transpose(W) + b\n",
    "\n",
    "#We can now make predictions for the movies that the user has not rated yet. \n",
    "#To recommend movies to a user, we can sort the predicted ratings in descending order and recommend the top N movies.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
